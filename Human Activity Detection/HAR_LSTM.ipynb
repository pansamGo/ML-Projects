{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).as_matrix()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return pd.get_dummies(y).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "epochs = 35                   \n",
    "batch_size = 32                  \n",
    "n_hidden = 256                  \n",
    "drop_out = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model with 1 layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 23:19:21.719681 11976 nn_ops.py:4224] Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               272384    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 273,926\n",
      "Trainable params: 273,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(drop_out))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 23:19:51.326107 11976 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1010 23:19:51.372969 11976 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 23:20:01.681306 11976 deprecation.py:323] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/35\n",
      "7352/7352 [==============================] - 142s 19ms/step - loss: 1.3411 - acc: 0.4002 - val_loss: 1.2544 - val_acc: 0.4768\n",
      "Epoch 2/35\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 1.2234 - acc: 0.4752 - val_loss: 1.5577 - val_acc: 0.3244\n",
      "Epoch 3/35\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 1.1260 - acc: 0.5276 - val_loss: 1.1605 - val_acc: 0.5087\n",
      "Epoch 4/35\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 1.1302 - acc: 0.5150 - val_loss: 0.8060 - val_acc: 0.6057\n",
      "Epoch 5/35\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.7858 - acc: 0.6295 - val_loss: 0.7274 - val_acc: 0.6488\n",
      "Epoch 6/35\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.6661 - acc: 0.7057 - val_loss: 0.6510 - val_acc: 0.7418\n",
      "Epoch 7/35\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.4786 - acc: 0.8263 - val_loss: 0.4258 - val_acc: 0.8602\n",
      "Epoch 8/35\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.3406 - acc: 0.8838 - val_loss: 0.3102 - val_acc: 0.8931\n",
      "Epoch 9/35\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.2619 - acc: 0.9127 - val_loss: 0.3605 - val_acc: 0.8992\n",
      "Epoch 10/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.2322 - acc: 0.9210 - val_loss: 0.3511 - val_acc: 0.8867\n",
      "Epoch 11/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.2069 - acc: 0.9280 - val_loss: 0.6683 - val_acc: 0.8527\n",
      "Epoch 12/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1941 - acc: 0.9354 - val_loss: 0.3235 - val_acc: 0.9053\n",
      "Epoch 13/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1800 - acc: 0.9363 - val_loss: 0.2559 - val_acc: 0.9074\n",
      "Epoch 14/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1847 - acc: 0.9354 - val_loss: 0.3230 - val_acc: 0.9080\n",
      "Epoch 15/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.2477 - acc: 0.9240 - val_loss: 0.2513 - val_acc: 0.9148\n",
      "Epoch 16/35\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.1635 - acc: 0.9402 - val_loss: 0.3573 - val_acc: 0.9074\n",
      "Epoch 17/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1527 - acc: 0.9479 - val_loss: 0.3652 - val_acc: 0.9016\n",
      "Epoch 18/35\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1545 - acc: 0.9446 - val_loss: 0.3818 - val_acc: 0.9053\n",
      "Epoch 19/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1592 - acc: 0.9431 - val_loss: 0.3257 - val_acc: 0.8951\n",
      "Epoch 20/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1404 - acc: 0.9456 - val_loss: 0.5671 - val_acc: 0.8734\n",
      "Epoch 21/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1400 - acc: 0.9463 - val_loss: 0.3576 - val_acc: 0.8894\n",
      "Epoch 22/35\n",
      "7352/7352 [==============================] - 130s 18ms/step - loss: 0.1385 - acc: 0.9479 - val_loss: 0.3086 - val_acc: 0.9257\n",
      "Epoch 23/35\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1698 - acc: 0.9457 - val_loss: 0.2936 - val_acc: 0.9108\n",
      "Epoch 24/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1461 - acc: 0.9440 - val_loss: 0.3550 - val_acc: 0.8989\n",
      "Epoch 25/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1493 - acc: 0.9456 - val_loss: 0.3228 - val_acc: 0.9155\n",
      "Epoch 26/35\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1283 - acc: 0.9498 - val_loss: 0.4807 - val_acc: 0.8728\n",
      "Epoch 27/35\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1207 - acc: 0.9512 - val_loss: 0.5020 - val_acc: 0.9084\n",
      "Epoch 28/35\n",
      "7352/7352 [==============================] - 131s 18ms/step - loss: 0.1180 - acc: 0.9527 - val_loss: 0.3718 - val_acc: 0.9046\n",
      "Epoch 29/35\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1385 - acc: 0.9495 - val_loss: 0.3655 - val_acc: 0.9162\n",
      "Epoch 30/35\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1372 - acc: 0.9504 - val_loss: 0.3705 - val_acc: 0.9216\n",
      "Epoch 31/35\n",
      "7352/7352 [==============================] - 150s 20ms/step - loss: 0.1530 - acc: 0.9501 - val_loss: 0.5002 - val_acc: 0.9006\n",
      "Epoch 32/35\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1535 - acc: 0.9505 - val_loss: 0.4029 - val_acc: 0.9009\n",
      "Epoch 33/35\n",
      "7352/7352 [==============================] - 144s 20ms/step - loss: 0.1345 - acc: 0.9508 - val_loss: 0.3555 - val_acc: 0.9152\n",
      "Epoch 34/35\n",
      "7352/7352 [==============================] - 147s 20ms/step - loss: 0.1319 - acc: 0.9479 - val_loss: 0.3275 - val_acc: 0.9199\n",
      "Epoch 35/35\n",
      "7352/7352 [==============================] - 141s 19ms/step - loss: 0.1498 - acc: 0.9495 - val_loss: 0.6165 - val_acc: 0.9060\n",
      "Time taken :  1:18:35.114140\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "True                                                                         \n",
      "LAYING                 537        0         0        0                   0   \n",
      "SITTING                  0      375       115        1                   0   \n",
      "STANDING                 0       57       473        0                   0   \n",
      "WALKING                  0        2         2      477                   6   \n",
      "WALKING_DOWNSTAIRS       0        0         1       37                 376   \n",
      "WALKING_UPSTAIRS         0        0         0       39                   0   \n",
      "\n",
      "Pred                WALKING_UPSTAIRS  \n",
      "True                                  \n",
      "LAYING                             0  \n",
      "SITTING                            0  \n",
      "STANDING                           2  \n",
      "WALKING                            9  \n",
      "WALKING_DOWNSTAIRS                 6  \n",
      "WALKING_UPSTAIRS                 432  \n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 9s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6165444772455196, 0.9060061079063454]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "epochs = 30                   \n",
    "batch_size = 32                  \n",
    "n_hidden = 512                  \n",
    "drop_out = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 00:51:38.137266 11976 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 512)               1069056   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,072,134\n",
      "Trainable params: 1,072,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(drop_out))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 629s 86ms/step - loss: 1.5089 - acc: 0.3402 - val_loss: 1.3403 - val_acc: 0.4092\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 649s 88ms/step - loss: 1.5295 - acc: 0.3716 - val_loss: 1.5504 - val_acc: 0.3739\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 629s 86ms/step - loss: 1.3803 - acc: 0.4115 - val_loss: 1.3213 - val_acc: 0.4174\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 547s 74ms/step - loss: 1.1767 - acc: 0.4973 - val_loss: 0.8094 - val_acc: 0.6030\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 289s 39ms/step - loss: 0.8387 - acc: 0.6269 - val_loss: 0.7467 - val_acc: 0.6977\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 286s 39ms/step - loss: 0.7446 - acc: 0.6771 - val_loss: 0.6599 - val_acc: 0.7119\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 296s 40ms/step - loss: 0.5344 - acc: 0.8040 - val_loss: 0.5986 - val_acc: 0.7536\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 295s 40ms/step - loss: 0.3996 - acc: 0.8655 - val_loss: 0.3836 - val_acc: 0.8704\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 297s 40ms/step - loss: 0.3145 - acc: 0.8942 - val_loss: 0.4376 - val_acc: 0.8575\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 284s 39ms/step - loss: 0.2822 - acc: 0.9037 - val_loss: 0.5159 - val_acc: 0.7900\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 288s 39ms/step - loss: 0.2558 - acc: 0.9158 - val_loss: 0.3717 - val_acc: 0.8863\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 295s 40ms/step - loss: 0.2101 - acc: 0.9255 - val_loss: 0.5566 - val_acc: 0.8890\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 354s 48ms/step - loss: 0.2035 - acc: 0.9305 - val_loss: 0.5502 - val_acc: 0.8700\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 286s 39ms/step - loss: 0.1760 - acc: 0.9329 - val_loss: 0.6543 - val_acc: 0.8738\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 283s 39ms/step - loss: 0.2171 - acc: 0.9350 - val_loss: 0.4370 - val_acc: 0.9023\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 387s 53ms/step - loss: 0.2031 - acc: 0.9295 - val_loss: 0.3562 - val_acc: 0.9023\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 562s 76ms/step - loss: 0.1515 - acc: 0.9418 - val_loss: 1.5394 - val_acc: 0.8086\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 561s 76ms/step - loss: 0.1612 - acc: 0.9426 - val_loss: 0.5296 - val_acc: 0.9033\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 597s 81ms/step - loss: 0.1517 - acc: 0.9430 - val_loss: 0.3856 - val_acc: 0.9087\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 603s 82ms/step - loss: 0.1406 - acc: 0.9455 - val_loss: 0.5601 - val_acc: 0.8985\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 600s 82ms/step - loss: 0.1743 - acc: 0.9423 - val_loss: 0.3276 - val_acc: 0.9230\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 595s 81ms/step - loss: 0.1485 - acc: 0.9475 - val_loss: 0.3797 - val_acc: 0.9070\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 594s 81ms/step - loss: nan - acc: 0.4944 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 589s 80ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 588s 80ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 591s 80ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 596s 81ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 588s 80ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 590s 80ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 594s 81ms/step - loss: nan - acc: 0.1668 - val_loss: nan - val_acc: 0.1683\n",
      "Time taken :  3:59:08.041579\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                WALKING\n",
      "True                       \n",
      "LAYING                  537\n",
      "SITTING                 491\n",
      "STANDING                532\n",
      "WALKING                 496\n",
      "WALKING_DOWNSTAIRS      420\n",
      "WALKING_UPSTAIRS        471\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 31s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.168306752629793\n"
     ]
    }
   ],
   "source": [
    "print(score[0])\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "epochs = 30                   \n",
    "batch_size = 32                  \n",
    "n_hidden = 128                  \n",
    "drop_out = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model_1 = Sequential()\n",
    "# Configuring the parameters\n",
    "model_1.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model_1.add(Dropout(drop_out))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model_1.add(Dense(n_classes, activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compiling the model\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 1.3071 - acc: 0.4188 - val_loss: 1.2370 - val_acc: 0.4123\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 1.1652 - acc: 0.4850 - val_loss: 1.1890 - val_acc: 0.4890\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.9696 - acc: 0.5760 - val_loss: 0.8628 - val_acc: 0.6468\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.7924 - acc: 0.6372 - val_loss: 0.8567 - val_acc: 0.5969\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.6695 - acc: 0.6858 - val_loss: 0.6848 - val_acc: 0.6780\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.5966 - acc: 0.7522 - val_loss: 0.5937 - val_acc: 0.8042\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.3786 - acc: 0.8674 - val_loss: 0.5302 - val_acc: 0.8198\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.2979 - acc: 0.8979 - val_loss: 0.3259 - val_acc: 0.8931\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.3337 - acc: 0.8795 - val_loss: 0.3682 - val_acc: 0.8901\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.2501 - acc: 0.9129 - val_loss: 0.2773 - val_acc: 0.8982\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.2554 - acc: 0.9067 - val_loss: 0.6779 - val_acc: 0.7635\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.2033 - acc: 0.9240 - val_loss: 0.3708 - val_acc: 0.8490\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1900 - acc: 0.9323 - val_loss: 0.2243 - val_acc: 0.9148\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1758 - acc: 0.9358 - val_loss: 0.2729 - val_acc: 0.8941\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1898 - acc: 0.9308 - val_loss: 0.3019 - val_acc: 0.9203\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1707 - acc: 0.9400 - val_loss: 0.2875 - val_acc: 0.9253\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1665 - acc: 0.9369 - val_loss: 0.2664 - val_acc: 0.9097\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1681 - acc: 0.9363 - val_loss: 0.2658 - val_acc: 0.9101\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 0.1488 - acc: 0.9433 - val_loss: 0.3019 - val_acc: 0.9121\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.1459 - acc: 0.9433 - val_loss: 0.2904 - val_acc: 0.9152\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1422 - acc: 0.9475 - val_loss: 0.2684 - val_acc: 0.9145\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1470 - acc: 0.9455 - val_loss: 0.2619 - val_acc: 0.9196\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1727 - acc: 0.9442 - val_loss: 0.2196 - val_acc: 0.9148\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1361 - acc: 0.9483 - val_loss: 0.2314 - val_acc: 0.9182\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1315 - acc: 0.9464 - val_loss: 0.2499 - val_acc: 0.9199\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1386 - acc: 0.9445 - val_loss: 0.2594 - val_acc: 0.9213\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1299 - acc: 0.9493 - val_loss: 0.3200 - val_acc: 0.9162\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1312 - acc: 0.9480 - val_loss: 0.3058 - val_acc: 0.9203\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1295 - acc: 0.9457 - val_loss: 0.3183 - val_acc: 0.9145\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1247 - acc: 0.9502 - val_loss: 0.3031 - val_acc: 0.9260\n",
      "Time taken :  0:14:24.212183\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model_1.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1 = model_1.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.3031002142316866\n",
      "Test Accuracy  0.9260264675941635\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score_1[0])\n",
    "print(\"Test Accuracy \" , score_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "epochs = 25                   \n",
    "batch_size = 128                  \n",
    "n_hidden = 512                  \n",
    "drop_out = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 20:37:39.842547 13684 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 512)               1069056   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,072,134\n",
      "Trainable params: 1,072,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model_1 = Sequential()\n",
    "# Configuring the parameters\n",
    "model_1.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model_1.add(Dropout(drop_out))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model_1.add(Dense(n_classes, activation='sigmoid'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 20:37:46.794115 13684 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1019 20:37:46.840937 13684 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Compiling the model\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 20:37:58.244091 13684 deprecation.py:323] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 355s 48ms/step - loss: 1.4378 - acc: 0.3539 - val_loss: 1.5373 - val_acc: 0.3488\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 361s 49ms/step - loss: 1.3609 - acc: 0.4019 - val_loss: 1.5678 - val_acc: 0.3420\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 362s 49ms/step - loss: 1.6731 - acc: 0.2920 - val_loss: 1.4541 - val_acc: 0.3414\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 357s 49ms/step - loss: 1.4820 - acc: 0.3636 - val_loss: 1.7327 - val_acc: 0.2050\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 361s 49ms/step - loss: 1.4496 - acc: 0.4004 - val_loss: 1.4869 - val_acc: 0.3580\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 405s 55ms/step - loss: 1.2548 - acc: 0.4641 - val_loss: 1.2893 - val_acc: 0.4917\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 361s 49ms/step - loss: 1.4773 - acc: 0.3862 - val_loss: 1.7715 - val_acc: 0.1890\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 369s 50ms/step - loss: 1.2936 - acc: 0.4557 - val_loss: 1.4750 - val_acc: 0.3845\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 352s 48ms/step - loss: 1.2332 - acc: 0.4815 - val_loss: 1.4504 - val_acc: 0.3655\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 343s 47ms/step - loss: 1.0042 - acc: 0.5492 - val_loss: 1.0693 - val_acc: 0.5117\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 168s 23ms/step - loss: 0.8862 - acc: 0.5947 - val_loss: 0.8713 - val_acc: 0.5955\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 168s 23ms/step - loss: 0.8103 - acc: 0.6318 - val_loss: 0.7218 - val_acc: 0.6980\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 170s 23ms/step - loss: 0.7430 - acc: 0.6801 - val_loss: 0.7840 - val_acc: 0.6413\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 166s 23ms/step - loss: 0.6529 - acc: 0.7398 - val_loss: 0.5445 - val_acc: 0.7998\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 167s 23ms/step - loss: 0.5400 - acc: 0.7982 - val_loss: 0.6705 - val_acc: 0.7516\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 167s 23ms/step - loss: 0.5816 - acc: 0.8138 - val_loss: 0.4972 - val_acc: 0.8375\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 169s 23ms/step - loss: 0.4046 - acc: 0.8672 - val_loss: 0.4473 - val_acc: 0.8198\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 171s 23ms/step - loss: 0.3540 - acc: 0.8828 - val_loss: 0.3230 - val_acc: 0.8741\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 172s 23ms/step - loss: 0.2690 - acc: 0.9070 - val_loss: 0.3315 - val_acc: 0.8639\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 172s 23ms/step - loss: 0.2292 - acc: 0.9138 - val_loss: 0.3364 - val_acc: 0.8890\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 169s 23ms/step - loss: 0.3755 - acc: 0.8957 - val_loss: 0.5657 - val_acc: 0.7964\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 172s 23ms/step - loss: 0.2502 - acc: 0.9119 - val_loss: 0.5957 - val_acc: 0.7771\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 170s 23ms/step - loss: 0.3712 - acc: 0.8890 - val_loss: 1.3730 - val_acc: 0.4079\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 178s 24ms/step - loss: 1.4716 - acc: 0.3187 - val_loss: 1.4622 - val_acc: 0.3081\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 173s 23ms/step - loss: 1.0056 - acc: 0.6070 - val_loss: 0.3739 - val_acc: 0.8673\n",
      "Time taken :  1:43:00.639062\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model_1.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_1 = model_1.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.37387920911103095\n",
      "Test Accuracy  0.8673227010519172\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score_1[0])\n",
    "print(\"Test Accuracy \" , score_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model with 2 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30                  \n",
    "batch_size= 32\n",
    "n_hidden_layer1 = 128\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.2\n",
    "drop_out_2 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 128, 128)          70656     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128)          512       \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 120,966\n",
      "Trainable params: 120,710\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model_2 = Sequential()\n",
    "# Configuring the parameters\n",
    "model_2.add(LSTM(n_hidden_layer1, return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model_2.add(Dropout(drop_out_1))\n",
    "# Adding batch normalization\n",
    "model_2.add(BatchNormalization())\n",
    "\n",
    "model_2.add(LSTM(n_hidden_layer2))\n",
    "# Adding a dropout layer\n",
    "model_2.add(Dropout(drop_out_2))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model_2.add(Dense(n_classes, activation='sigmoid'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 110s 15ms/step - loss: 0.7840 - acc: 0.7444 - val_loss: 0.4422 - val_acc: 0.8663\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.3214 - acc: 0.9034 - val_loss: 0.4218 - val_acc: 0.8537\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.2339 - acc: 0.9226 - val_loss: 0.2183 - val_acc: 0.9203\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1915 - acc: 0.9319 - val_loss: 0.2653 - val_acc: 0.9053\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 136s 19ms/step - loss: 0.1769 - acc: 0.9385 - val_loss: 0.2450 - val_acc: 0.9067\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 140s 19ms/step - loss: 0.1631 - acc: 0.9380 - val_loss: 0.2529 - val_acc: 0.8982\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 145s 20ms/step - loss: 0.1516 - acc: 0.9423 - val_loss: 0.2489 - val_acc: 0.9189\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.1495 - acc: 0.9434 - val_loss: 0.2659 - val_acc: 0.9084\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1442 - acc: 0.9431 - val_loss: 0.3140 - val_acc: 0.9030\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1442 - acc: 0.9440 - val_loss: 0.1962 - val_acc: 0.9311\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1331 - acc: 0.9437 - val_loss: 0.3383 - val_acc: 0.8982\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 132s 18ms/step - loss: 0.1342 - acc: 0.9460 - val_loss: 0.2190 - val_acc: 0.9237\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1272 - acc: 0.9480 - val_loss: 0.2696 - val_acc: 0.9094\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1330 - acc: 0.9475 - val_loss: 0.2646 - val_acc: 0.9162\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1294 - acc: 0.9493 - val_loss: 0.3072 - val_acc: 0.9226\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1289 - acc: 0.9490 - val_loss: 0.2725 - val_acc: 0.9141\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1235 - acc: 0.9512 - val_loss: 0.3073 - val_acc: 0.9335\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1259 - acc: 0.9499 - val_loss: 0.2598 - val_acc: 0.9247\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1203 - acc: 0.9506 - val_loss: 0.2656 - val_acc: 0.9203\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1313 - acc: 0.9498 - val_loss: 0.2134 - val_acc: 0.9301\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1172 - acc: 0.9532 - val_loss: 0.3513 - val_acc: 0.8867\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1375 - acc: 0.9486 - val_loss: 0.3127 - val_acc: 0.9158\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1176 - acc: 0.9531 - val_loss: 0.2179 - val_acc: 0.9199\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1229 - acc: 0.9508 - val_loss: 0.2534 - val_acc: 0.9267\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1163 - acc: 0.9521 - val_loss: 0.2754 - val_acc: 0.9209\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1193 - acc: 0.9513 - val_loss: 0.2492 - val_acc: 0.9189\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1109 - acc: 0.9550 - val_loss: 0.2193 - val_acc: 0.9372\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1190 - acc: 0.9525 - val_loss: 0.3456 - val_acc: 0.9253\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 133s 18ms/step - loss: 0.1084 - acc: 0.9548 - val_loss: 0.3434 - val_acc: 0.9118\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1205 - acc: 0.9538 - val_loss: 0.2833 - val_acc: 0.9243\n",
      "Time taken :  1:06:56.079307\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model_2.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_2 = model_2.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.28329760189572595\n",
      "Test Accuracy  0.9243298269426535\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score_2[0])\n",
    "print(\"Test Accuracy \" , score_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50                \n",
    "batch_size= 64\n",
    "n_hidden_layer1 = 32\n",
    "n_hidden_layer2 =64\n",
    "drop_out_1 = 0.5\n",
    "drop_out_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 32)           128       \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 30,726\n",
      "Trainable params: 30,662\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Initiliazing the sequential model\n",
    "model_3 = Sequential()\n",
    "# Configuring the parameters\n",
    "model_3.add(LSTM(n_hidden_layer1, return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model_3.add(Dropout(drop_out_1))\n",
    "# Adding batch normalization\n",
    "model_3.add(BatchNormalization())\n",
    "\n",
    "model_3.add(LSTM(n_hidden_layer2))\n",
    "# Adding a dropout layer\n",
    "model_3.add(Dropout(drop_out_2))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model_3.add(Dense(n_classes, activation='sigmoid'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 1.1300 - acc: 0.5647 - val_loss: 0.9103 - val_acc: 0.6525\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 58s 8ms/step - loss: 0.7584 - acc: 0.6930 - val_loss: 0.7881 - val_acc: 0.6451\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.5752 - acc: 0.7553 - val_loss: 0.6252 - val_acc: 0.7723\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.4374 - acc: 0.8455 - val_loss: 0.4508 - val_acc: 0.8473\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3366 - acc: 0.8964 - val_loss: 0.4305 - val_acc: 0.8490\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2598 - acc: 0.9193 - val_loss: 0.3325 - val_acc: 0.8748\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2251 - acc: 0.9214 - val_loss: 0.3800 - val_acc: 0.8809\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2043 - acc: 0.9272 - val_loss: 0.3478 - val_acc: 0.8884\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1899 - acc: 0.9314 - val_loss: 0.3101 - val_acc: 0.8996\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1803 - acc: 0.9357 - val_loss: 0.2658 - val_acc: 0.9063\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1639 - acc: 0.9378 - val_loss: 0.2617 - val_acc: 0.9111\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1631 - acc: 0.9434 - val_loss: 0.3102 - val_acc: 0.9162\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1570 - acc: 0.9418 - val_loss: 0.3238 - val_acc: 0.8992\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1431 - acc: 0.9476 - val_loss: 0.2106 - val_acc: 0.9267\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1448 - acc: 0.9468 - val_loss: 0.3298 - val_acc: 0.9057\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1367 - acc: 0.9479 - val_loss: 0.3993 - val_acc: 0.8982\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1368 - acc: 0.9476 - val_loss: 0.3545 - val_acc: 0.9111\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1318 - acc: 0.9493 - val_loss: 0.2902 - val_acc: 0.9162\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1266 - acc: 0.9486 - val_loss: 0.3716 - val_acc: 0.9030\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1290 - acc: 0.9506 - val_loss: 0.3003 - val_acc: 0.9155\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1330 - acc: 0.9476 - val_loss: 0.3965 - val_acc: 0.9002\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1276 - acc: 0.9491 - val_loss: 0.3151 - val_acc: 0.9002\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1319 - acc: 0.9474 - val_loss: 0.3492 - val_acc: 0.9189\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1191 - acc: 0.9520 - val_loss: 0.3965 - val_acc: 0.9050\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1247 - acc: 0.9524 - val_loss: 0.3338 - val_acc: 0.9250\n",
      "Epoch 26/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1182 - acc: 0.9517 - val_loss: 0.3678 - val_acc: 0.9189\n",
      "Epoch 27/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1192 - acc: 0.9501 - val_loss: 0.3613 - val_acc: 0.9101\n",
      "Epoch 28/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1165 - acc: 0.9543 - val_loss: 0.2805 - val_acc: 0.9179\n",
      "Epoch 29/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1170 - acc: 0.9531 - val_loss: 0.3283 - val_acc: 0.9277\n",
      "Epoch 30/50\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1201 - acc: 0.9513 - val_loss: 0.2991 - val_acc: 0.9233\n",
      "Epoch 31/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1139 - acc: 0.9521 - val_loss: 0.3705 - val_acc: 0.9148\n",
      "Epoch 32/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1133 - acc: 0.9525 - val_loss: 0.4109 - val_acc: 0.9070\n",
      "Epoch 33/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1149 - acc: 0.9548 - val_loss: 0.3748 - val_acc: 0.9138\n",
      "Epoch 34/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1159 - acc: 0.9516 - val_loss: 0.4341 - val_acc: 0.9141\n",
      "Epoch 35/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1096 - acc: 0.9555 - val_loss: 0.4302 - val_acc: 0.9104\n",
      "Epoch 36/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1141 - acc: 0.9532 - val_loss: 0.3597 - val_acc: 0.9270\n",
      "Epoch 37/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1155 - acc: 0.9532 - val_loss: 0.4360 - val_acc: 0.9036\n",
      "Epoch 38/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1089 - acc: 0.9546 - val_loss: 0.3867 - val_acc: 0.9057\n",
      "Epoch 39/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1057 - acc: 0.9551 - val_loss: 0.4418 - val_acc: 0.9226\n",
      "Epoch 40/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1044 - acc: 0.9547 - val_loss: 0.4893 - val_acc: 0.9108\n",
      "Epoch 41/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1030 - acc: 0.9546 - val_loss: 0.4605 - val_acc: 0.9233\n",
      "Epoch 42/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1039 - acc: 0.9576 - val_loss: 0.4518 - val_acc: 0.9158\n",
      "Epoch 43/50\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 0.1023 - acc: 0.9554 - val_loss: 0.4676 - val_acc: 0.9175\n",
      "Epoch 44/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.1059 - acc: 0.9548 - val_loss: 0.5450 - val_acc: 0.9074\n",
      "Epoch 45/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0935 - acc: 0.9585 - val_loss: 0.4906 - val_acc: 0.9121\n",
      "Epoch 46/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0996 - acc: 0.9558 - val_loss: 0.5014 - val_acc: 0.9101\n",
      "Epoch 47/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0985 - acc: 0.9592 - val_loss: 0.5265 - val_acc: 0.9125\n",
      "Epoch 48/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0981 - acc: 0.9612 - val_loss: 0.4949 - val_acc: 0.9267\n",
      "Epoch 49/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0963 - acc: 0.9614 - val_loss: 0.4187 - val_acc: 0.9237\n",
      "Epoch 50/50\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0951 - acc: 0.9601 - val_loss: 0.4538 - val_acc: 0.9152\n",
      "Time taken :  0:42:33.388274\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model_3.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25                \n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1020 11:39:41.144582  4124 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1020 11:39:41.472668  4124 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1020 11:39:41.472668  4124 deprecation.py:506] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1020 11:39:41.472668  4124 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1020 11:39:41.519531  4124 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 103,556\n",
      "Trainable params: 103,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1020 11:39:48.689685  4124 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1020 11:39:48.736552  4124 deprecation_wrapper.py:119] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1020 11:39:52.860619  4124 deprecation.py:323] From c:\\users\\acer\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.7660 - acc: 0.6827 - val_loss: 0.6110 - val_acc: 0.7811\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 6s 880us/step - loss: 0.3490 - acc: 0.8629 - val_loss: 0.4754 - val_acc: 0.8480\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 6s 875us/step - loss: 0.2435 - acc: 0.9063 - val_loss: 0.3927 - val_acc: 0.8670\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 6s 875us/step - loss: 0.1837 - acc: 0.9310 - val_loss: 0.3400 - val_acc: 0.8924\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 6s 884us/step - loss: 0.1522 - acc: 0.9406 - val_loss: 0.3324 - val_acc: 0.8870\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 7s 905us/step - loss: 0.1385 - acc: 0.9423 - val_loss: 0.3143 - val_acc: 0.8958\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 6s 880us/step - loss: 0.1210 - acc: 0.9501 - val_loss: 0.2878 - val_acc: 0.9111\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 6s 875us/step - loss: 0.1155 - acc: 0.9506 - val_loss: 0.2957 - val_acc: 0.9009\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 6s 873us/step - loss: 0.1123 - acc: 0.9533 - val_loss: 0.3122 - val_acc: 0.9026\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 6s 871us/step - loss: 0.1045 - acc: 0.9538 - val_loss: 0.3223 - val_acc: 0.9036\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 7s 908us/step - loss: 0.0998 - acc: 0.9558 - val_loss: 0.3564 - val_acc: 0.8982\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 7s 900us/step - loss: 0.0955 - acc: 0.9596 - val_loss: 0.3132 - val_acc: 0.9074\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 6s 873us/step - loss: 0.0933 - acc: 0.9584 - val_loss: 0.3201 - val_acc: 0.9077\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 7s 902us/step - loss: 0.1010 - acc: 0.9561 - val_loss: 0.3365 - val_acc: 0.9009\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 7s 903us/step - loss: 0.0894 - acc: 0.9599 - val_loss: 0.3479 - val_acc: 0.9040\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 7s 916us/step - loss: 0.0799 - acc: 0.9653 - val_loss: 0.3701 - val_acc: 0.9074\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 7s 903us/step - loss: 0.0788 - acc: 0.9663 - val_loss: 0.3970 - val_acc: 0.9070\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 6s 882us/step - loss: 0.0818 - acc: 0.9648 - val_loss: 0.3827 - val_acc: 0.9030\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 7s 901us/step - loss: 0.0829 - acc: 0.9626 - val_loss: 0.4073 - val_acc: 0.9053\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 7s 909us/step - loss: 0.0736 - acc: 0.9679 - val_loss: 0.3879 - val_acc: 0.9080\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 7s 907us/step - loss: 0.0702 - acc: 0.9702 - val_loss: 0.3952 - val_acc: 0.9111\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 7s 892us/step - loss: 0.0782 - acc: 0.9668 - val_loss: 0.4166 - val_acc: 0.8836\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 7s 907us/step - loss: 0.0759 - acc: 0.9653 - val_loss: 0.3377 - val_acc: 0.9097\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 7s 905us/step - loss: 0.0694 - acc: 0.9680 - val_loss: 0.3837 - val_acc: 0.9046\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 7s 903us/step - loss: 0.0682 - acc: 0.9697 - val_loss: 0.4052 - val_acc: 0.8955\n",
      "Time taken :  0:02:46.306998\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.40516277702925346\n",
      "Test Accuracy  0.8954869358669834\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score[0])\n",
    "print(\"Test Accuracy \" , score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25                \n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 124, 128)          5888      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 120, 64)           41024     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 120, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 57, 64)            16448     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 57, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3648)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                182450    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 246,116\n",
      "Trainable params: 246,116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 1.5818 - acc: 0.3770 - val_loss: 1.0215 - val_acc: 0.6427\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.6450 - acc: 0.7067 - val_loss: 0.7343 - val_acc: 0.6356\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.4303 - acc: 0.8186 - val_loss: 0.5463 - val_acc: 0.8127\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.2856 - acc: 0.8857 - val_loss: 0.5511 - val_acc: 0.8045\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.2004 - acc: 0.9208 - val_loss: 0.4848 - val_acc: 0.8415\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1779 - acc: 0.9301 - val_loss: 0.4217 - val_acc: 0.8856\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1595 - acc: 0.9392 - val_loss: 0.4178 - val_acc: 0.8853\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1364 - acc: 0.9453 - val_loss: 0.4497 - val_acc: 0.8707\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1316 - acc: 0.9455 - val_loss: 0.3743 - val_acc: 0.8911\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1248 - acc: 0.9461 - val_loss: 0.3724 - val_acc: 0.8985\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1199 - acc: 0.9474 - val_loss: 0.3797 - val_acc: 0.8853\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1164 - acc: 0.9509 - val_loss: 0.4762 - val_acc: 0.8361\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1332 - acc: 0.9396 - val_loss: 0.3518 - val_acc: 0.9030\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1110 - acc: 0.9513 - val_loss: 0.3452 - val_acc: 0.9070\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1077 - acc: 0.9538 - val_loss: 0.3818 - val_acc: 0.9060\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1082 - acc: 0.9540 - val_loss: 0.3839 - val_acc: 0.8951\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1056 - acc: 0.9536 - val_loss: 0.3387 - val_acc: 0.9097\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1286 - acc: 0.9498 - val_loss: 0.3939 - val_acc: 0.8867\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1270 - acc: 0.9516 - val_loss: 0.3276 - val_acc: 0.9155\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1156 - acc: 0.9510 - val_loss: 0.3560 - val_acc: 0.8921\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 0.1015 - acc: 0.9551 - val_loss: 0.3366 - val_acc: 0.9111\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.0992 - acc: 0.9540 - val_loss: 0.3152 - val_acc: 0.9179\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.0966 - acc: 0.9561 - val_loss: 0.3435 - val_acc: 0.9026\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.0967 - acc: 0.9569 - val_loss: 0.3432 - val_acc: 0.9196\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.0872 - acc: 0.9606 - val_loss: 0.3525 - val_acc: 0.9169\n",
      "Time taken :  0:11:25.867490\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.3524832843419216\n",
      "Test Accuracy  0.9168646080760094\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score[0])\n",
    "print(\"Test Accuracy \" , score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40                \n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 123, 246)          13530     \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 119, 128)          157568    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 119, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 54, 64)            49216     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 54, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                221248    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 441,952\n",
      "Trainable params: 441,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=246, kernel_size=6, activation='relu',kernel_initializer='random_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.88))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=6, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.89))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 1.1683 - acc: 0.4629 - val_loss: 0.8387 - val_acc: 0.6624\n",
      "Epoch 2/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.5551 - acc: 0.7514 - val_loss: 0.6162 - val_acc: 0.7978\n",
      "Epoch 3/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2940 - acc: 0.8890 - val_loss: 0.4443 - val_acc: 0.8907\n",
      "Epoch 4/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.1718 - acc: 0.9342 - val_loss: 0.4105 - val_acc: 0.8972\n",
      "Epoch 5/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1437 - acc: 0.9421 - val_loss: 0.5445 - val_acc: 0.8833\n",
      "Epoch 6/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.1345 - acc: 0.9425 - val_loss: 0.4486 - val_acc: 0.9033\n",
      "Epoch 7/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1231 - acc: 0.9490 - val_loss: 0.2757 - val_acc: 0.9175\n",
      "Epoch 8/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.1351 - acc: 0.9482 - val_loss: 0.2920 - val_acc: 0.9294\n",
      "Epoch 9/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.1989 - acc: 0.9440 - val_loss: 0.3191 - val_acc: 0.9220\n",
      "Epoch 10/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2332 - acc: 0.9446 - val_loss: 0.3474 - val_acc: 0.9165\n",
      "Epoch 11/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2224 - acc: 0.9480 - val_loss: 0.3186 - val_acc: 0.9281\n",
      "Epoch 12/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2189 - acc: 0.9480 - val_loss: 0.3465 - val_acc: 0.9084\n",
      "Epoch 13/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2245 - acc: 0.9444 - val_loss: 0.3633 - val_acc: 0.9247\n",
      "Epoch 14/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2130 - acc: 0.9480 - val_loss: 0.3928 - val_acc: 0.9342\n",
      "Epoch 15/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2102 - acc: 0.9480 - val_loss: 0.4068 - val_acc: 0.9230\n",
      "Epoch 16/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2158 - acc: 0.9475 - val_loss: 0.4732 - val_acc: 0.9152\n",
      "Epoch 17/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.3718 - acc: 0.9241 - val_loss: 0.4865 - val_acc: 0.8839\n",
      "Epoch 18/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2894 - acc: 0.9343 - val_loss: 0.5460 - val_acc: 0.9036\n",
      "Epoch 19/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2736 - acc: 0.9411 - val_loss: 0.5387 - val_acc: 0.9121\n",
      "Epoch 20/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2329 - acc: 0.9421 - val_loss: 0.4620 - val_acc: 0.9203\n",
      "Epoch 21/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2325 - acc: 0.9426 - val_loss: 0.5261 - val_acc: 0.9155\n",
      "Epoch 22/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2760 - acc: 0.9391 - val_loss: 0.5391 - val_acc: 0.9179\n",
      "Epoch 23/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1747 - acc: 0.9490 - val_loss: 0.5266 - val_acc: 0.9165\n",
      "Epoch 24/40\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.1451 - acc: 0.9524 - val_loss: 0.5046 - val_acc: 0.9148\n",
      "Epoch 25/40\n",
      "7352/7352 [==============================] - 67s 9ms/step - loss: 0.1696 - acc: 0.9508 - val_loss: 0.3547 - val_acc: 0.9220\n",
      "Epoch 26/40\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 0.1570 - acc: 0.9484 - val_loss: 0.4063 - val_acc: 0.9223\n",
      "Epoch 27/40\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 0.3695 - acc: 0.9312 - val_loss: 0.6576 - val_acc: 0.8707\n",
      "Epoch 28/40\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.3114 - acc: 0.9366 - val_loss: 0.3791 - val_acc: 0.9186\n",
      "Epoch 29/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2377 - acc: 0.9455 - val_loss: 0.4866 - val_acc: 0.9128\n",
      "Epoch 30/40\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.2316 - acc: 0.9504 - val_loss: 0.4326 - val_acc: 0.9114\n",
      "Epoch 31/40\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.2582 - acc: 0.9468 - val_loss: 0.3488 - val_acc: 0.9315\n",
      "Epoch 32/40\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2443 - acc: 0.9494 - val_loss: 0.3880 - val_acc: 0.9250\n",
      "Epoch 33/40\n",
      "7352/7352 [==============================] - 67s 9ms/step - loss: 0.2383 - acc: 0.9476 - val_loss: 0.3674 - val_acc: 0.9233\n",
      "Epoch 34/40\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.2317 - acc: 0.9504 - val_loss: 0.3841 - val_acc: 0.9209\n",
      "Epoch 35/40\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.2029 - acc: 0.9532 - val_loss: 0.4248 - val_acc: 0.9192\n",
      "Epoch 36/40\n",
      "7352/7352 [==============================] - 67s 9ms/step - loss: 0.2109 - acc: 0.9521 - val_loss: 0.5454 - val_acc: 0.9216\n",
      "Epoch 37/40\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.2366 - acc: 0.9502 - val_loss: 0.4279 - val_acc: 0.9216\n",
      "Epoch 38/40\n",
      "7352/7352 [==============================] - 67s 9ms/step - loss: 0.2220 - acc: 0.9551 - val_loss: 0.4350 - val_acc: 0.9192\n",
      "Epoch 39/40\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 0.2051 - acc: 0.9548 - val_loss: 0.4016 - val_acc: 0.9199\n",
      "Epoch 40/40\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1992 - acc: 0.9551 - val_loss: 0.4385 - val_acc: 0.9216\n",
      "Time taken :  0:43:00.707163\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  0.4384836606237519\n",
      "Test Accuracy  0.9216152019002375\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score[0])\n",
    "print(\"Test Accuracy \" , score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30                \n",
    "batch_size= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 123, 512)          28160     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 119, 256)          655616    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 119, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 59, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 54, 128)           196736    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 54, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 49, 128)           98432     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 49, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,380,806\n",
      "Trainable params: 1,380,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=512, kernel_size=6, activation='relu',kernel_initializer='random_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=256, kernel_size=5, activation='relu',kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.88))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=6, activation='relu',kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Conv1D(filters=128, kernel_size=6, activation='relu',kernel_initializer='random_uniform'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 215s 29ms/step - loss: 12.0574 - acc: 0.1881 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 213s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 213s 29ms/step - loss: 13.0240 - acc: 0.1919 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 213s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 213s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 217s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 222s 30ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 222s 30ms/step - loss: 13.0357 - acc: 0.1912 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 227s 31ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 219s 30ms/step - loss: 13.0313 - acc: 0.1915 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 216s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 217s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 217s 29ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 224s 30ms/step - loss: 13.0291 - acc: 0.1916 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 227s 31ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 223s 30ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 138s 19ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.1626 - acc: 0.1832 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 105s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 105s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 104s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 103s 14ms/step - loss: 13.0335 - acc: 0.1914 - val_loss: 13.1811 - val_acc: 0.1822\n",
      "Time taken :  1:23:07.805060\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)\n",
    "\n",
    "print(\"Time taken : \", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score  13.181068860517748\n",
      "Test Accuracy  0.18221920597217509\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score \" , score[0])\n",
    "print(\"Test Accuracy \" , score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prettytable for layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------------+---------+------------+\n",
      "| Example | Epochs | Batch Size | Hidden Layer | Dropout | Accuracy % |\n",
      "+---------+--------+------------+--------------+---------+------------+\n",
      "|    1    |   35   |     32     |     256      |   0.65  |    90.6    |\n",
      "|    2    |   30   |     32     |     512      |   0.8   |   16.84    |\n",
      "|    3    |   30   |     32     |     128      |   0.5   |    92.6    |\n",
      "|    4    |   25   |    128     |     512      |   0.8   |   86.73    |\n",
      "+---------+--------+------------+--------------+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "number       = [1, 2, 3, 4]\n",
    "epochs       = [35, 30, 30,25]\n",
    "batch_size   = [32, 32, 32,128]\n",
    "n_hidden     = [256, 512, 128,512]\n",
    "drop_out     = [0.65, 0.80, 0.50,0.80]\n",
    "accuracy     = [90.60, 16.84, 92.60, 86.73]\n",
    "\n",
    "# Initializing prettytable \n",
    "ptable = PrettyTable()\n",
    "ptable.add_column(\"Example\",number)\n",
    "ptable.add_column(\"Epochs\", epochs)\n",
    "ptable.add_column(\"Batch Size\",batch_size)\n",
    "ptable.add_column(\"Hidden Layer\",n_hidden) \n",
    "ptable.add_column(\"Dropout\",drop_out) \n",
    "ptable.add_column(\"Accuracy %\",accuracy) \n",
    "print(ptable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prettytable for layer 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+----------------+----------------+-----------+-----------+------------+\n",
      "| Example | Epochs | Batch Size | Hidden Layer 1 | Hidden Layer 2 | Dropout 1 | Dropout 2 | Accuracy % |\n",
      "+---------+--------+------------+----------------+----------------+-----------+-----------+------------+\n",
      "|    1    |   30   |     32     |      128       |       64       |    0.2    |    0.5    |   92.43    |\n",
      "|    2    |   50   |     64     |       32       |       64       |    0.5    |    0.5    |   91.52    |\n",
      "+---------+--------+------------+----------------+----------------+-----------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "number          = [1, 2]\n",
    "epochs          = [30, 50]\n",
    "batch_size      = [32, 64]\n",
    "n_hidden_layer1 = [128, 32]\n",
    "n_hidden_layer2 = [64, 64]\n",
    "drop_out_1      = [0.2, 0.5]\n",
    "drop_out_2      = [0.5, 0.5]\n",
    "accuracy     = [92.43, 91.52]\n",
    "\n",
    "# Initializing prettytable \n",
    "ptable = PrettyTable()\n",
    "ptable.add_column(\"Example\",number)\n",
    "ptable.add_column(\"Epochs\", epochs)\n",
    "ptable.add_column(\"Batch Size\",batch_size)\n",
    "ptable.add_column(\"Hidden Layer 1\",n_hidden_layer1)\n",
    "ptable.add_column(\"Hidden Layer 2\",n_hidden_layer2)\n",
    "ptable.add_column(\"Dropout 1\",drop_out_1)\n",
    "ptable.add_column(\"Dropout 2\",drop_out_2)\n",
    "ptable.add_column(\"Accuracy %\",accuracy) \n",
    "print(ptable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prettytable for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+------------+\n",
      "| Model | Epochs | Batch Size | Accuracy % |\n",
      "+-------+--------+------------+------------+\n",
      "|   1   |   25   |    128     |   89.54    |\n",
      "|   2   |   25   |    128     |   91.68    |\n",
      "|   3   |   40   |    128     |   92.16    |\n",
      "|   4   |   30   |    128     |   18.22    |\n",
      "+-------+--------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "number          = [1, 2, 3, 4]\n",
    "epochs          = [25, 25, 40, 30]\n",
    "batch_size      = [128, 128, 128, 128]\n",
    "accuracy     = [89.54,91.68,92.16,18.22]\n",
    "\n",
    "# Initializing prettytable \n",
    "ptable = PrettyTable()\n",
    "ptable.add_column(\"Model\",number)\n",
    "ptable.add_column(\"Epochs\", epochs)\n",
    "ptable.add_column(\"Batch Size\",batch_size)\n",
    "ptable.add_column(\"Accuracy %\",accuracy) \n",
    "print(ptable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We have used one LSTM layered model and two LSTM layered model.\n",
    "2. Used different epochs, batch sizes, dropout layers to find accuracy.\n",
    "3. Maximum accuracy % in one LSTM layer model is 92.60%\n",
    "4. We also used larger LSTM units with large dropout rates in one layered model.\n",
    "4. Accuracy % in two LSTM layers model is 92.43%\n",
    "5. Earlystopping technique can help to get maximum accuracy of 93.72% in two layered architecture.\n",
    "6. We have tried all different combinations of epochs, batch sizes and dropout values to find maximum accuracy\n",
    "   and approx 93% is best accuracy.\n",
    "7. With different CNN models, we get best accuracy of 92.16%. We have used different architectures of Conv2D with \n",
    "   different kernel sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
