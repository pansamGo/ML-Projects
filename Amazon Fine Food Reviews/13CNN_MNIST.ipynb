{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, BatchNormalization, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1970705,
     "status": "ok",
     "timestamp": 1524577821923,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "//lh3.googleusercontent.com/-EsJzSyawCkQ/AAAAAAAAAAI/AAAAAAAAAWk/jhKHALKaHag/s50-c-k-no/photo.jpg",
      "userId": "116292885805316472049"
     },
     "user_tz": -330
    },
    "id": "H9EU0e8yzFOm",
    "outputId": "b9a7331d-3a52-4991-f4a2-eaa64e288eb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 01:32:51.226855 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 01:32:51.320584 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 01:32:51.320584 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 01:32:51.383068 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0809 01:32:51.383068 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0809 01:32:51.398689 12008 deprecation.py:506] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 01:32:51.523696 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 01:32:51.539281 12008 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0809 01:32:51.711116 12008 deprecation.py:323] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 0.2648 - acc: 0.9184 - val_loss: 0.0651 - val_acc: 0.9781\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0873 - acc: 0.9739 - val_loss: 0.0409 - val_acc: 0.9862\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0671 - acc: 0.9799 - val_loss: 0.0366 - val_acc: 0.9872\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0552 - acc: 0.9830 - val_loss: 0.0321 - val_acc: 0.9891\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0464 - acc: 0.9856 - val_loss: 0.0296 - val_acc: 0.9904\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0428 - acc: 0.9872 - val_loss: 0.0304 - val_acc: 0.9899\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0381 - acc: 0.9888 - val_loss: 0.0261 - val_acc: 0.9919\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0345 - acc: 0.9893 - val_loss: 0.0286 - val_acc: 0.9919\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0299 - acc: 0.9902 - val_loss: 0.0280 - val_acc: 0.9913\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0306 - acc: 0.9908 - val_loss: 0.0287 - val_acc: 0.9915\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0277 - acc: 0.9915 - val_loss: 0.0286 - val_acc: 0.9903\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0274 - acc: 0.9917 - val_loss: 0.0248 - val_acc: 0.9925\n",
      "Test loss: 0.024807610545822537\n",
      "Test accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 16:04:03.157577  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 37s 618us/step - loss: 0.1461 - acc: 0.9548 - val_loss: 0.0566 - val_acc: 0.9806\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 40s 663us/step - loss: 0.0490 - acc: 0.9848 - val_loss: 0.0411 - val_acc: 0.9877\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 39s 656us/step - loss: 0.0345 - acc: 0.9890 - val_loss: 0.0384 - val_acc: 0.9875\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 40s 663us/step - loss: 0.0246 - acc: 0.9921 - val_loss: 0.0346 - val_acc: 0.9897\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 40s 666us/step - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0359 - val_acc: 0.9891\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 39s 649us/step - loss: 0.0162 - acc: 0.9944 - val_loss: 0.0400 - val_acc: 0.9889\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 40s 670us/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0401 - val_acc: 0.9871\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 41s 678us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0370 - val_acc: 0.9895\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 39s 654us/step - loss: 0.0096 - acc: 0.9968 - val_loss: 0.0398 - val_acc: 0.9886\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 41s 675us/step - loss: 0.0081 - acc: 0.9974 - val_loss: 0.0368 - val_acc: 0.9910\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 40s 672us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0406 - val_acc: 0.9902\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 39s 657us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 0.0363 - val_acc: 0.9902\n",
      "Test loss: 0.03634111389952359\n",
      "Test accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Conv2D(32, kernel_size=(2, 2),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_1.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model_1.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_1.add(BatchNormalization())\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(111, activation='relu'))\n",
    "model_1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_1 = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_1[0])\n",
    "print('Test accuracy:', score_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.2875 - acc: 0.9204 - val_loss: 0.0532 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 185s 3ms/step - loss: 0.0909 - acc: 0.9750 - val_loss: 0.0283 - val_acc: 0.9908\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.0649 - acc: 0.9825 - val_loss: 0.0255 - val_acc: 0.9919\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 188s 3ms/step - loss: 0.0532 - acc: 0.9850 - val_loss: 0.0365 - val_acc: 0.9895\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.0447 - acc: 0.9874 - val_loss: 0.0211 - val_acc: 0.9924\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0383 - acc: 0.9892 - val_loss: 0.0249 - val_acc: 0.9924\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0339 - acc: 0.9906 - val_loss: 0.0158 - val_acc: 0.9952\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0323 - acc: 0.9906 - val_loss: 0.0210 - val_acc: 0.9927\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0277 - acc: 0.9921 - val_loss: 0.0159 - val_acc: 0.9952\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0257 - acc: 0.9930 - val_loss: 0.0153 - val_acc: 0.9957\n",
      "Test loss: 0.015346395371133803\n",
      "Test accuracy: 0.9957\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_2.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_2.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(BatchNormalization())\n",
    "model_2.add(Dropout(0.30))\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_2 = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_2[0])\n",
    "print('Test accuracy:', score_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/11\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.2035 - acc: 0.9440 - val_loss: 0.0379 - val_acc: 0.9887\n",
      "Epoch 2/11\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.0680 - acc: 0.9806 - val_loss: 0.0320 - val_acc: 0.9899\n",
      "Epoch 3/11\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.0490 - acc: 0.9860 - val_loss: 0.0330 - val_acc: 0.9881\n",
      "Epoch 4/11\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.0405 - acc: 0.9882 - val_loss: 0.0217 - val_acc: 0.9921\n",
      "Epoch 5/11\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 0.0361 - acc: 0.9893 - val_loss: 0.0212 - val_acc: 0.9929\n",
      "Epoch 6/11\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.0291 - acc: 0.9915 - val_loss: 0.0162 - val_acc: 0.9945\n",
      "Epoch 7/11\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.0271 - acc: 0.9916 - val_loss: 0.0196 - val_acc: 0.9930\n",
      "Epoch 8/11\n",
      "60000/60000 [==============================] - 130s 2ms/step - loss: 0.0233 - acc: 0.9927 - val_loss: 0.0289 - val_acc: 0.9897\n",
      "Epoch 9/11\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0173 - val_acc: 0.9948\n",
      "Epoch 10/11\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.0210 - acc: 0.9937 - val_loss: 0.0172 - val_acc: 0.9951\n",
      "Epoch 11/11\n",
      "60000/60000 [==============================] - 146s 2ms/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0187 - val_acc: 0.9949\n",
      "Test loss: 0.018701013443106058\n",
      "Test accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "epochs = 11\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Conv2D(64, kernel_size=(6, 6),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model_3.add(Conv2D(32, (6, 6), activation='relu'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(Dropout(0.25))\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(128, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_3.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_3 = model_3.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_3[0])\n",
    "print('Test accuracy:', score_3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 14:12:37.660238  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 14:12:37.691510  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 14:12:37.707100  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 14:12:37.753999  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0809 14:12:37.769587  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0809 14:12:37.785243  2872 deprecation.py:506] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0809 14:12:37.785243  2872 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0809 14:12:38.035182  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0809 14:12:38.050771  2872 deprecation_wrapper.py:119] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0809 14:12:38.222638  2872 deprecation.py:323] From C:\\Users\\ACER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 1.8476 - acc: 0.3396 - val_loss: 0.2950 - val_acc: 0.9105\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.2793 - acc: 0.9129 - val_loss: 0.0936 - val_acc: 0.9705\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.1542 - acc: 0.9513 - val_loss: 0.0637 - val_acc: 0.9786\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 116s 2ms/step - loss: 0.1222 - acc: 0.9622 - val_loss: 0.0517 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.1033 - acc: 0.9673 - val_loss: 0.0424 - val_acc: 0.9854\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 123s 2ms/step - loss: 0.0941 - acc: 0.9703 - val_loss: 0.0405 - val_acc: 0.9860\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 123s 2ms/step - loss: 0.0834 - acc: 0.9738 - val_loss: 0.0377 - val_acc: 0.9865\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 118s 2ms/step - loss: 0.0784 - acc: 0.9759 - val_loss: 0.0352 - val_acc: 0.9878\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.0756 - acc: 0.9768 - val_loss: 0.0338 - val_acc: 0.9876\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.0703 - acc: 0.9778 - val_loss: 0.0309 - val_acc: 0.9897\n",
      "Test loss: 0.03086712787185097\n",
      "Test accuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "# With activation = softmax, batch normalization and dropout layer\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(Conv2D(32, kernel_size=(4, 4),\n",
    "                 activation='sigmoid',\n",
    "                 input_shape=input_shape))\n",
    "model_4.add(Conv2D(64, (4, 4), activation='sigmoid'))\n",
    "model_4.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_4.add(Dropout(0.70))\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(128, activation='sigmoid'))\n",
    "model_4.add(BatchNormalization(epsilon=0.001))\n",
    "model_4.add(Dropout(0.30))\n",
    "model_4.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_4.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_4 = model_4.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_4[0])\n",
    "print('Test accuracy:', score_4[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.3902 - acc: 0.8877 - val_loss: 0.0984 - val_acc: 0.9726\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.1223 - acc: 0.9660 - val_loss: 0.0655 - val_acc: 0.9803\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.0921 - acc: 0.9726 - val_loss: 0.0489 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 0.0792 - acc: 0.9763 - val_loss: 0.0460 - val_acc: 0.9851\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.0707 - acc: 0.9786 - val_loss: 0.0422 - val_acc: 0.9863\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.0606 - acc: 0.9817 - val_loss: 0.0390 - val_acc: 0.9877\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.0577 - acc: 0.9824 - val_loss: 0.0356 - val_acc: 0.9881\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.0516 - acc: 0.9839 - val_loss: 0.0349 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 62s 1ms/step - loss: 0.0484 - acc: 0.9849 - val_loss: 0.0310 - val_acc: 0.9899\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 62s 1ms/step - loss: 0.0462 - acc: 0.9855 - val_loss: 0.0304 - val_acc: 0.9902\n",
      "Test loss: 0.03044679508442059\n",
      "Test accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# With activation = tanh, without batch normalization, with dropout, with adam optimizer\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model_5 = Sequential()\n",
    "model_5.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='tanh',\n",
    "                 input_shape=input_shape))\n",
    "model_5.add(Conv2D(32, (3, 3), activation='tanh'))\n",
    "model_5.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_5.add(Dropout(0.44))\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(128, activation='sigmoid'))\n",
    "model_5.add(Dropout(0.30))\n",
    "model_5.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_5.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_5.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_5 = model_5.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_5[0])\n",
    "print('Test accuracy:', score_5[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/11\n",
      "60000/60000 [==============================] - 194s 3ms/step - loss: 0.2787 - acc: 0.9100 - val_loss: 0.0591 - val_acc: 0.9823\n",
      "Epoch 2/11\n",
      "60000/60000 [==============================] - 192s 3ms/step - loss: 0.0592 - acc: 0.9824 - val_loss: 0.0470 - val_acc: 0.9850\n",
      "Epoch 3/11\n",
      "60000/60000 [==============================] - 195s 3ms/step - loss: 0.0469 - acc: 0.9859 - val_loss: 0.0407 - val_acc: 0.9870\n",
      "Epoch 4/11\n",
      "60000/60000 [==============================] - 191s 3ms/step - loss: 0.0396 - acc: 0.9885 - val_loss: 0.0395 - val_acc: 0.9869\n",
      "Epoch 5/11\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.0345 - acc: 0.9905 - val_loss: 0.0362 - val_acc: 0.9881\n",
      "Epoch 6/11\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.0310 - acc: 0.9913 - val_loss: 0.0355 - val_acc: 0.9884\n",
      "Epoch 7/11\n",
      "60000/60000 [==============================] - 189s 3ms/step - loss: 0.0282 - acc: 0.9920 - val_loss: 0.0341 - val_acc: 0.9883\n",
      "Epoch 8/11\n",
      "60000/60000 [==============================] - 191s 3ms/step - loss: 0.0259 - acc: 0.9928 - val_loss: 0.0343 - val_acc: 0.9878\n",
      "Epoch 9/11\n",
      "60000/60000 [==============================] - 192s 3ms/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.0351 - val_acc: 0.9886\n",
      "Epoch 10/11\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 0.0223 - acc: 0.9942 - val_loss: 0.0327 - val_acc: 0.9879\n",
      "Epoch 11/11\n",
      "60000/60000 [==============================] - 203s 3ms/step - loss: 0.0209 - acc: 0.9944 - val_loss: 0.0334 - val_acc: 0.9880\n",
      "Test loss: 0.03338987519220682\n",
      "Test accuracy: 0.988\n"
     ]
    }
   ],
   "source": [
    "# Activation = selu & softmax , without Batch normalization and dropout, Initializer = kernel_initializer &\n",
    "# bias_initializer, Optimizer = Adagrad\n",
    "\n",
    "epochs = 11\n",
    "\n",
    "model_6 = Sequential()\n",
    "model_6.add(Conv2D(64, kernel_size=(5, 5),\n",
    "                 activation='selu',\n",
    "                 input_shape=input_shape))\n",
    "model_6.add(Dense(64,\n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'))\n",
    "model_6.add(Conv2D(32, (4, 4), activation='softmax'))\n",
    "model_6.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_6.add(Flatten())\n",
    "model_6.add(Dense(128, activation='selu'))\n",
    "model_6.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adagrad(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_6.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_6 = model_6.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_6[0])\n",
    "print('Test accuracy:', score_6[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.6562 - acc: 0.7877 - val_loss: 0.2706 - val_acc: 0.9254\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.2479 - acc: 0.9251 - val_loss: 0.1361 - val_acc: 0.9570\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.1668 - acc: 0.9492 - val_loss: 0.0909 - val_acc: 0.9698\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1280 - acc: 0.9607 - val_loss: 0.1309 - val_acc: 0.9565\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.1124 - acc: 0.9657 - val_loss: 0.0902 - val_acc: 0.9704\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.1011 - acc: 0.9692 - val_loss: 0.0525 - val_acc: 0.9821\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0914 - acc: 0.9715 - val_loss: 0.0498 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0839 - acc: 0.9738 - val_loss: 0.0506 - val_acc: 0.9832\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0793 - acc: 0.9754 - val_loss: 0.0546 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.0748 - acc: 0.9765 - val_loss: 0.0498 - val_acc: 0.9841\n",
      "Test loss: 0.04983391810744069\n",
      "Test accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "#Activation = softplus, Initializer = random_normal, with dropout and batch normalization, Optimizer = Adamax\n",
    "from keras import initializers\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model_7 = Sequential()\n",
    "model_7.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='softplus',\n",
    "                 input_shape=input_shape))\n",
    "model_7.add(Conv2D(32, (4, 4), activation='softplus'))\n",
    "model_7.add(Dense(32,\n",
    "                kernel_initializer=initializers.random_normal(stddev=0.01)))\n",
    "model_7.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model_7.add(Dropout(0.50))\n",
    "model_7.add(Flatten())\n",
    "model_7.add(Dense(128, activation='softplus'))\n",
    "model_7.add(BatchNormalization(epsilon=0.001))\n",
    "model_7.add(Dropout(0.30))\n",
    "model_7.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_7.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adamax(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_7.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score_7 = model_7.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score_7[0])\n",
    "print('Test accuracy:', score_7[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prettytable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------------------+---------------+\n",
      "| Index |  Model  |      Test Loss       | Test Accuracy |\n",
      "+-------+---------+----------------------+---------------+\n",
      "|   1   | Model 1 | 0.03634111389952359  |     0.9902    |\n",
      "|   2   | Model 2 | 0.015346395371133803 |     0.9957    |\n",
      "|   3   | Model 3 | 0.018701013443106058 |     0.9949    |\n",
      "|   4   | Model 4 | 0.03086712787185097  |     0.9897    |\n",
      "|   5   | Model 5 | 0.03044679508442059  |     0.9902    |\n",
      "|   6   | Model 6 | 0.03338987519220682  |     0.988     |\n",
      "|   7   | Model 7 | 0.04983391810744069  |     0.9841    |\n",
      "+-------+---------+----------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "number= [1,2,3,4,5,6,7] \n",
    "name= [\"Model 1\",\"Model 2\",\"Model 3\",\"Model 4\",\"Model 5\",\"Model 6\",\"Model 7\"] \n",
    "loss= [score_1[0],score_2[0],score_3[0],score_4[0],score_5[0],score_6[0],score_7[0]] \n",
    "acc= [score_1[1],score_2[1],score_3[1],score_4[1],score_5[1],score_6[1],score_7[1]]       \n",
    "\n",
    "#Initialize Prettytable \n",
    "ptable = PrettyTable() \n",
    "ptable.add_column(\"Index\", number) \n",
    "ptable.add_column(\"Model\", name) \n",
    "ptable.add_column(\"Test Loss\", loss) \n",
    "ptable.add_column(\"Test Accuracy\", acc) \n",
    "print(ptable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For MNIST dataset, we build Convolutional Neural Networks using Conv2D.\n",
    "2. We have used different kernel sizes like (3,3),(4,4),(5,5) etc.\n",
    "3. Different layers are used in all the examples.\n",
    "4. Some models with or without Batch normalization and Dropout layers to check the perforamnce.\n",
    "5. Activation methods like softplus, relu, selu, sigmoid etc. are used\n",
    "6. Optimizers like Adagrad, Adam, Adamax, Adadelta are used.\n",
    "7. Test accuracy is almost same in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
